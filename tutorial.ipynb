{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -J -O \"https://is.gd/tsdlIV\" -L -J -O \"https://is.gd/C3lSUl\" -L -J -O \"https://is.gd/UVXwPO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all.pkl', 'rb') as f:\n",
    "    ids, speakers, all_labels, embeddings, audio, visual, \\\n",
    "        raw_sentences, trains_index, tests_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3281"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many videos are there?\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22029"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many video segments are there?\n",
    "np.sum([len(segments) for segments in ids.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--qXJuDtHPw', '-3g5yACwYnA', '-3nNcZdcdvU', '-571d8cVauQ', '-6rXp3zJ3kc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ids.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-3g5yACwYnA[0]',\n",
       " '-3g5yACwYnA[1]',\n",
       " '-3g5yACwYnA[2]',\n",
       " '-3g5yACwYnA[3]',\n",
       " '-3g5yACwYnA[4]',\n",
       " '-3g5yACwYnA[5]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids['-3g5yACwYnA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"operations key sp polymer sp brings a sp technical aspect to sp our sp operation that we don't have sp internally sp we're\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences['-3g5yACwYnA'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can generate the word list from a specific sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['operations', 'key', 'sp', 'polymer', 'sp', 'brings', 'a', 'sp', 'technical', 'aspect', 'to', 'sp', 'our', 'sp', 'operation', 'that', 'we', \"don't\", 'have', 'sp', 'internally', 'sp', \"we're\"]\n"
     ]
    }
   ],
   "source": [
    "print((raw_sentences['-3g5yACwYnA'][3]).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/embedding_dict.pkl', 'rb') as f:\n",
    "    embedding_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.014424 , -0.0060105, -0.23573  ,  0.055827 ,  0.16621  ,\n",
       "         0.08586  ,  0.056778 , -0.12082  , -0.2068   ,  1.8227   ]),\n",
       " (300,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict['word'][:10], embedding_dict['word'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can set the dimension of embedding to 300:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embeddings = len(list(embedding_dict.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'solutions sp we have many new opportunities through sp the way things have changed through the years sp looking for new niche high end value added products sp'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences['-3g5yACwYnA'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = (raw_sentences['-3g5yACwYnA'][5]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embedding = np.zeros(dim_embeddings)\n",
    "word_counts = len(all_words)\n",
    "missed_word_counts = 0\n",
    "\n",
    "for word in all_words:\n",
    "    # of course, similarly you can do the replacing jobs\n",
    "    try:\n",
    "        new_embedding += embedding_dict[word]\n",
    "    except KeyError:\n",
    "        missed_word_counts += 1\n",
    "\n",
    "if word_counts != missed_word_counts:\n",
    "    new_embedding /= (len(all_words) - missed_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(new_embedding, embeddings['-3g5yACwYnA'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample: generating sentiments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/labels_index.dict', 'rb') as f:\n",
    "    index_of = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_label_dict(name, labels=all_labels, index_of=index_of):\n",
    "    '''\n",
    "    generate specific emotion/sentiment label dict from the labels dict\n",
    "    '''\n",
    "    label_dict = {}\n",
    "    label_index = index_of[name]\n",
    "\n",
    "    for key, label_list in labels.items():\n",
    "        for label in label_list:\n",
    "            try:\n",
    "                label_dict[key].append(label[label_index])\n",
    "            except KeyError:\n",
    "                label_dict[key] = [label[label_index]]\n",
    "\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = get_emotion_label_dict('sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can export it to a valid `.pkl` file for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(filename, labels):\n",
    "    '''\n",
    "    export data to a file\n",
    "    '''\n",
    "    path = 'data/' + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        all_modalities = (\n",
    "            ids, speakers, labels, embeddings, audio, visual,\n",
    "            raw_sentences, trains_index, tests_index)\n",
    "\n",
    "        pickle.dump(all_modalities, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data('categorical', sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python train_categorical.py --lr=1e-4 --l2=1e-5 --rec-dropout=0.1 --dropout=0.5 --batch-size=128 --epochs=20 --log_dir='logs/mosei_categorical'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aeb043183a806feef8623284bda7ec95709c10ea061383b34305649f9f5c2d96"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cogsci2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
